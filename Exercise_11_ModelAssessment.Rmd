Model Assessment
========================================================

In this activity we will use a series of visualizations and statistical measures to assess the performance of our Super Simple Ecosystem Model at the Metolius site.

Let's start by loading the ensemble output from the previous lab and the observed flux data for the site.
```{r}
## load libraries
#install.packages("randomForest")
#install.packages("plotrix")
library("plotrix")
library(rpart)
library(randomForest)

## load SSEM output
load("Ex10.output.RData")

## load flux tower data
L4 = read.csv("data/AMF_USMe2_2005_L4_h_V002.txt",header=TRUE,na.strings="-9999")
L4[L4==-9999] = NA
```

Sanity Check
------------

When assessing model performance, one can often diagnose bugs in the code and other large errors without the need to make a direct model-data comparison, simply by looking at basic statistics and diagnostic graphs. Also, it is not uncommon to have model outputs for quantities that are not directly observed, but which should be checked to make sure they make sense and that the model is not producing the right answer somewhere else for the wrong reason. In the code below we look at the daily-mean outputs from the unweighted ensemble (output.ensemble) and the resampled particle filter (output)

```{r}
ciEnvelope <- function(x,ylo,yhi,...){
  polygon(cbind(c(x, rev(x), x[1]), c(ylo, rev(yhi),
                                      ylo[1])), border = NA,...) 
}
col.alpha <- function(col,alpha=1){
  rgb = col2rgb(col)
  rgb(rgb[1],rgb[2],rgb[3],alpha*255,maxColorValue=255)
}
varnames <- c("Bleaf","Bwood","BSOM","LAI","NEP","GPP","Ra","NPPw","NPPl","Rh","litter","CWD")
units <- c("Mg/ha","Mg/ha","Mg/ha","m2/m2","umol/m2/sec","umol/m2/sec","umol/m2/sec","umol/m2/sec","umol/m2/sec","umol/m2/sec","Mg/ha/timestep","Mg/ha/timestep")

## Time-series visualization, daily means
DoY = floor(L4$DoY-0.02)
uDoY = sort(unique(DoY))
ci = list(pf=list(),ens=list())
for(i in 1:12){
  ci.pf  = apply(apply(output[,,i],2,tapply,DoY,mean),1,quantile,c(0.025,0.5,0.975))
  ci.ens = apply(apply(output.ensemble[,,i],2,tapply,DoY,mean),1,quantile,c(0.025,0.5,0.975))
#  ci.pf = apply(output[,,i],1,quantile,c(0.025,0.5,0.975))
#  ci.ens = apply(output.ensemble[,,i],1,quantile,c(0.025,0.5,0.975))
  plot(uDoY,ci.ens[2,],main=varnames[i],xlab="time",ylab=units[i],type='l',ylim=range(ci.ens))
  ciEnvelope(uDoY,ci.ens[1,],ci.ens[3,],col=col.alpha("lightGrey",0.5))
  ciEnvelope(uDoY,ci.pf[1,],ci.pf[3,],col=col.alpha("lightGreen",0.5))
  lines(uDoY,ci.ens[2,])
  lines(uDoY,ci.pf[2,],col=3)
  #lines(uDoY, O)
  ci$pf[[i]] = ci.pf
  ci$ens[[i]] = ci.ens
}

```

Question 1: Do the predicted pools and fluxes make sense? Are the signs correct? Are the seasonal patterns reasonable? Is the magnitude of the pools & fluxes reasonable? 

> CWD (coarse woody debris) represents the dead trees or fallen tree limbs on the forest floor.  This plot makes sense as increased CWD would be due to the either introduction of disease or insect or events such as fires, storms, or floods.  Assuming that this ecosystem is not subject to seasonal wildfires, storms, disease, or insects, our expectation would be that CWD is positive and has a small positive slope that reflects the natural mortality rate.  The CI from the particle filter is very large in the beginning which could be due to high initial condition uncertaity.

> Bleaf (tau.leaf IC) represents the initial condition leaf biomass in the forest.  We would expect there to be seasonal patterns if the forest is deciduous.  The predicted value of Bleaf by the ensemble diverges from the PF in the 3rd and fourth quarter of the year.  The ensemble mean increases for Bleaf while the PF mean decreases for Bleaf.  One expects Bleaf to decrease starting in the fall and stay low through the winter until spring begins and Bleaf increases.  This doesn't make sense because I expect that the ensemble and PF would move with the same sign but with potentially different magnitudes.  

> Bwood (tau.wood IC) represents the initial condition of wood biomass in the forest.  The predicted Bwood makes sense because it is slowly increasing over the course of a year, with certain seasons seeing faster growth than others.  This makes sense because trees would grow more/faster when they have leafs and performing photosynthesis (summer, spring, and early fall). These seasonal patterns in addition to the magnitude are reasonable. 

> BSOM (tau.soil organic matter IC) represents the initial condition of soil organic matter in the forest.  One would expect to see BSOM to be lower in the colder months because microorganisms wont be as productive.  This expected decrease is visible in the BSOM plot towards the end of the year. The pf mean has two large dips in what we expect to be the winter months.  Although a decrease in BSOM is expected, the magnitude of these decreaeses does not appear reasonable.  Additionall there is a sharp increase during this same season which we would not expect.

> LAI (leaf area index) represents the canopy cover of the forest.  The predicted LAI makes sense as we would expect it to increase starting in the spring and decrease in the fall (assuming a deciduous forest).  We observe distinct periods of seasonal increase and decrease in the ensemble and PF; albeit at different times of the year.  The ensemle mean is lowest during the beginning of the year and increase towards the end of the year.  Contrastingly, the PF decreases towards the end of the year and is therefore larger in the beginning of the year.  This divergence towards the end of the year makes the seasonal patterns of the ensemble unreasonable but the PF patterns reasonable. Fall and winter occur at the end of the year so we would expect decreases in LAI, which we observe in the PF, not increases, which we observe in the LAI.

> The NEP (net ecosystem productivity), NPPw (net primary production of wood), and  NPPl (net primary production of leaves) fluxs are all reasonable because they each exhibit diurnal and seasonal cycle.s  NEP, NPPw, and NPPl increase during the day and are greater in the late spring, summer, and beginning of fall because leaf productivity is greater with more exposure to sunlight in warm temperatures.  This increased NPPl contirbutes to NPPw and NPPe  When there is not sunlight, at night, and during cold seasons, when plants dont have their leaves, we expect NEP, NPPw, and NPPl to decrease.  Therefore what we observe in these plots is reasonable with regards to their pattern, signs, and magnitudes.  Finally, we observe that the CI for both the ensemble and PF to be greater when NEP, NPPw, and NPPl are prediced to be higher and have smaller CI when they are predicted to have slower rates.  

> GPP (gross primary productivity) is the rate at which photosynthesis occurs.  This value cannot be below zero so the predicted values of GPP do not make sense.  We would expect to see predictions for GPP to exhibit diurnal and seasonal patterns similar to NEP: higher during the day and warmer months while lower at night and during the colder months.  However, the signs for both the ensemle and predicted GPP means are below zero at night and during the colder months. This is unreasonable.  For the PF we also observe greater uncertainty in the beginning of the year which may reflect initial condition uncertainty.

> Ra (soil respiration) The predicted Ra fluxes are reasonable because Ra has both a dirunal and seasonal cycle.  Ra is greater in warmer temperatures which occur during the day when the sun it out and during the warmer months.  We observe this in the daily increases of Ra while the sum is out and the long term trend of increasing Ra during the warmer seasons (end of spring, summer, and early fall).  All the values of Ra are positive which is what we would expect and the magnitudes are also reasonable.


> Rh (relative humidity) is the proportion of water vapor in the air over the saturation potential (total possible amount of water vapor the air could hold).  As temperature increases the saturation potential also increases which decreases the relative humidity.  The predicted Rh, its' signs, seasonality, and magnitude, reasonable.  As temperatures increase in the warmer months the saturation potential of the air increases which one could expect to decrease the Rh, but if the forest is in a humid climate one would expect Rh to increase as well.  This increased saturation potential contirbutes to the increased predicted Rh and Rh uncertainty we observe in the warmer months.  During colder season and at night, we expect Rh to be smaller and the CI to be smaller.  Therefore the predictions are reasonable with regards to the sign (there cant be negative humidity), seasonality, and magnitude.

> The predicted Litter make sense because we expect litter to increase in the fall, when leaves fall from trees in deciduous forests.  This seasonality is observed in the predicted means from the ensemble but not from the PF.  The ensemble and PF diverge towards the end of the year. The predicted means from the ensemble are reasonable in that the sign is positive which is what we would expect: an increase in leaf litter during the season when the leaves fall.  Contrastingly, the PF mean predictions decrease which is not the sign we expect.
 

Model vs. Data
--------------

In the following section we will begin with some basic diagnostic plots and statistics assessing the predicted NEE by our simple ecosystem model. Specifically, we will calculate the Root Mean Square Error (RMSE), bias, correlation coefficient, and regression slopes of the relationship between the observed and predicted NEE for both the original ensemble and the particle filter. We will also generate scatter plots of predicted vs. observed values.

```{r}

## Calculate ensemble means & apply QAQC
qaqc = (L4$qf_NEE_st == 0)
NEE.ens = -apply(output.ensemble[,,5],1,mean)
NEE.pf  = -apply(output[,,5],1,mean)
E = NEE.ens[qaqc]
P = NEE.pf[qaqc]
O = L4$NEE_st_fMDS[qaqc]
length(L4$NEE_st_fMDS[qaqc])

## Model vs obs regressions
NEE.ens.fit = lm(O ~ E)
NEE.pf.fit = lm(O ~ P)

## performance stats
stats = as.data.frame(matrix(NA,4,2))
rownames(stats) <- c("RMSE","Bias","cor","slope")
colnames(stats) <- c("ens","pf")
stats["RMSE",'ens'] = sqrt(mean((E-O)^2))
stats["RMSE",'pf']  = sqrt(mean((P-O)^2))
stats['Bias','ens'] = mean(E-O)
stats['Bias','pf']  = mean(P-O)
stats['cor','ens']  = cor(E,O)
stats['cor','pf']   = cor(P,O)
stats['slope','ens'] = coef(NEE.ens.fit)[2]
stats['slope','pf']  = coef(NEE.pf.fit)[2]
knitr::kable(stats)

## predicted-observed
plot(E,O,pch=".",xlab="ensemble",ylab='observed',main='NEE (umol/m2/sec)')
abline(0,1,col=2,lwd=2)
abline(NEE.ens.fit,col=3,lwd=3,lty=2)
legend("bottomright",legend=c('obs','1:1','reg'),col=1:3,lwd=3)

plot(P,O,pch=".",xlab="particle filter",ylab='observed',main='NEE (umol/m2/sec)')
abline(0,1,col=2,lwd=2)
abline(NEE.pf.fit,col=3,lwd=3,lty=2)
legend("bottomright",legend=c('obs','1:1','reg'),col=1:3,lwd=3)



```

Question 2: Which version of the model performed better? Do the statistics or plots give any indication about what parameters might need to be fixed, or processeses refined in the model?

> The PF performed slightly better than the Ensemble according to our diagnostic statistics and plots.  The PF had smaller standard deviation on the prediction errors indicating that the model was more accurate than the ensemble.  Both models had a negative bias, they underpredicted NEE, but the PF had a smaller bias/underprediction.  The PF had a correlation coefficient .01 higher than the ensembles' coefficient for NEE.  The regression slope for the PF was closer to the observed slope than the ensemble slope was by about .03.  Comparing these statistics and the regression plots we can conclude that the PF performed better. Additionally, these statistics and plot show that both models improved each iteration.  This indicates that initical condition uncertainty could be improved in both models to increase model performance overall.
the standard deviation of the residuals (prediction errors)

Question 3: Repeat the daily-mean time-series plot for NEE from the previous section, but add the observed daily-mean NEE to the plot. Make sure to use the gap-filled NEE estimates, since flux data are not missing at random.
```{r}
## Time-series visualization, daily means
DoY = floor(L4$DoY-0.02)
uDoY = sort(unique(DoY))
ci = list(pf=list(),ens=list())
#for(i in 1:12){
i = 5
  ci.pf  = apply(apply(output[,,i],2,tapply,DoY,mean),1,quantile,c(0.025,0.5,0.975))
  ci.ens = apply(apply(output.ensemble[,,i],2,tapply,DoY,mean),1,quantile,c(0.025,0.5,0.975))
#}
O = L4$NEE_st_fMDS
NEE.O.daily = tapply(O, floor(L4$DoY-0.02), mean)
plot(uDoY,ci.ens[2,],main=varnames[i],xlab="time",ylab=units[i],type='l',ylim=range(ci.ens))
  ciEnvelope(uDoY,ci.ens[1,],ci.ens[3,],col=col.alpha("lightGrey",0.5))
  ciEnvelope(uDoY,ci.pf[1,],ci.pf[3,],col=col.alpha("lightGreen",0.5))
  lines(uDoY,ci.ens[2,])
  lines(uDoY,ci.pf[2,],col=3)
  lines(uDoY, -NEE.O.daily)
  ci$pf[[i]] = ci.pf
  ci$ens[[i]] = ci.ens
```
> We observe NEE decrease during the same time periods we observe NEP to increase: the late spring, summer, and early fall seasons.  This is due to the sequestration of carbon by plants during the photosynthetic process.  Therefore as photosynthesis and productivity increase, lhe net ecosystem exchange of carbon will decrease.  We also observe the NEE and NEP exhibit diurnal patterns, but their fluxes are opposite of one another.  As NEP decreases plants will uptake less carbon so the NEE will increase/become more positive.


Comparison to flux "climatology"
-------------------------------

In the section below we calculate the long-term average NEE for each 30 min period in the year, excluding the year we modeled (2005) as an alternative model to judge our process model against. We then update our summary statistics and predicted-observed plot

```{r}
## flux "climatology"
fluxfiles = dir("data",pattern="AMF")
fluxfiles = fluxfiles[grep("txt",fluxfiles)]
fluxfiles = fluxfiles[-grep("2005",fluxfiles)]
clim.NEE = clim.doy = NULL
for(f in fluxfiles){
  ff = read.csv(file.path("data",f),header=TRUE,na.strings="-9999")
  ff[ff == -9999] = NA
  clim.NEE = c(clim.NEE,ff$NEE_st_fMDS)
  clim.doy = c(clim.doy,ff$DoY)
}
NEE.clim=tapply(clim.NEE,clim.doy,mean,na.rm=TRUE)[1:length(qaqc)]
C = NEE.clim[qaqc]
NEE.clim.fit = lm(O ~ C)
NEE.E.fit = lm(O~E)
NEE.P.fit = lm(O~P)
summary(NEE.clim.fit)
summary(NEE.E.fit)
summary(NEE.P.fit)
stats["RMSE",3]  = sqrt(mean((C-O)^2))
stats['Bias',3]  = mean(C-O)
stats['cor',3]   = cor(C,O)
stats['slope',3] = coef(NEE.clim.fit)[2]
colnames(stats)[3] <- "clim"
knitr::kable(stats)
plot(C,O,pch=".",xlab="climatology",ylab='observed',main='NEE (umol/m2/sec)')
abline(0,1,col=2,lwd=2)
abline(NEE.clim.fit,col=3,lwd=3,lty=2)
legend("bottomright",legend=c('obs','1:1','reg'),col=1:3,lwd=3)

## example cycle
plot(L4$DoY,-L4$NEE_st_fMDS,xlim=c(200,210),type='l',lwd=2,ylim=c(-10,20),xlab="Day of Year",ylab="NEE")
lines(L4$DoY,-NEE.clim,col=4,lwd=2,lty=2)
legend("topright",legend=c("Obs","clim"),lty=1:2,col=c(1,4),lwd=2)

```

Question 4: How does the process model perform relative to the average flux data? Which statistics showed the largest differences between the model and climatology? 

> The process model that uses the long-term average of NEE at 30 min granularity performs the best according the RMSE, R2, Bias, and slope diagnostics.  The process model's predictions are closer to the observed values and has a RMSE that is more than 50% smaller than the Ensemle and Particle Filter.  The R2 of the model is .6 compared to the other models .4 which indicates that the process model has a greater explanatory power with regardes to variability.  Additionally, the bias of the model is 0.19 compared to the -5 values from the ensemble and particle filter.  The slope of the process model is nearly 1 (.97) and so closely follows the 1:1 line on the predicted vs. observed plot.  All of these conclusions indicate that the process model is more precise and accurate in its predictions.  Comparitively, the superiority of the process model to the ensemble and particle filter is most pronounced in its smaller RMSE and bias.  The smaller spread of residuals in the process model indicates that this model is more precise (able to repeat result reliably).  The bias of ensemble and particle filter informs us that these models underestimate NEE by a lot relative to the process model which slightly overestimates the NEE.  Interestingly, we observe that the process model tenseds to overestimate in earlier observation while the ensemble and particle filter underestimated the most in the earlier observation.  This could reflect that the ensemble and particle filter have greater initial condition uncertainty which could be due to poorly estimated pool sizes for the priors.   

Taylor diagram
--------------

Next, let's use a Taylor diagram to pull our summary statistics together into one plot. One of the advantages of the Taylor diagram is that it makes it simpler to visually diagnose the relative differences in model performance, especially when comparing multiple models or different versions of the same model. In the figure below we'll begin by plotting the ensemble, the particle filter, and the climatology. While not common, the Taylor diagram also provides a way of expressing model and data uncertainty in the plot by plotting ensemble estimates of both. Below we add all 200 members of the model ensemble, as well as a Monte Carlo estimate of observation error in the flux data. The latter is derived based on the research by Richardson et al (2006), who showed that eddy covariance data has a non-symmetric heteroskedastic, Laplace distribution. The non-symmetric part refers to the fact that there is greater error in positive fluxes (= respiration, typically nocturnal measurments) than in negative ones.

```{r}
## Taylor diagrams
taylor.diagram(ref=O,model=E,normalize=TRUE,ref.sd=TRUE)
taylor.diagram(ref=O,model=P,add=TRUE,normalize=TRUE,col=3)
taylor.diagram(ref=O,model=C,add=TRUE,normalize=TRUE,col=4)

# Thin black line is RMSE, 
# Thick black line is magnitude of standard deviation compared to observe value
# dashed lines represent correlation between the model and observations


## add full ensemble
for(i in 1:ncol(output)){
  taylor.diagram(ref=O,model=-output.ensemble[qaqc,i,5],col=2,pch=".",add=TRUE,normalize=TRUE)
}

## add data uncertainty
rlaplace = function(n,mu,b){
  return(mu + ifelse(rbinom(n,1,0.5),1,-1)*rexp(n,b))
}
beta = ifelse(O > 0,0.62+0.63*O,1.42-0.19*O) #Heteroskedasticity, parameters from Richardson et al 2006
for(i in 1:200){
  x = rlaplace(length(O),O,beta)
  taylor.diagram(ref=O,model=x,col=5,add=TRUE,normalize=TRUE)
}
legend("topright",legend=c("ens","PF","clim","obsUncert"),col=2:5,pch=20,cex=0.7)
```

Question 5: What did you learn about model performance from the Taylor diagram? 

> From the Taylor diagram I observed how observation uncertainty and each ensemble member compare to the process model, ensemble, and particle filter.  The MC estimate of observation error has a smaller RMSE (small black line) than all three models and all ensemble members.  The observation uncertainty has the highest correlation with observations and a standard deviation of 1 compared to the observed values which means uncertainty follows the observation through time with high precision and accuracy.  This coud be attributed to a systemic data collection bias/measurement error that would affect both the observation and their uncertainty.  

> Assessing the ensemble models on the Taylor diagram we can determine that there is a large spread with regards to RMSE, standard deviations, and correlation. Knowing that both the ensemble model and particle filter improved as time went on, due to either ensemble adjusted or weighting of members based on likelihood, we can determine that member closer to the origin are generated later in time and members farther from the origin are from earlier.  As ensembles are adjusted they nudge the model closer to the observed data.  This results in a decreased RMSE and standard deviation relative to the observed data.  We also observe that ensemble members do not improve with regards to correlation with the model data.  The members do not move outside of the dashed-line sectors indicating zone of correlation.

Question 6: How do our simple models and flux climatology compare to the ensemble of ecosystem models in Figure 7 of Schwalm et al 2010  "A model-data intercomparison of CO2 exchange across North America: results from the north american carbon program site synthesis". J. Geophys. Res. ?

> Our flux climdatology model would fall very closely to a cluster of benchmarks  (orbserved normalized monthly NEE) such as K and m, and mean models such as P and S.  From looking at this diagram by Schwalm the flux model would have an average performance across all three diagnostics (correlation, standard deviation, and RMSE) relative to the other benchmarks and models.  Its' correlation is on the higher side, and it is well within the 1 markers for both RMSE and Standard deviation like most of the benchmarks and plots.

> Our simple model performs quite poorly relative to the benchmarks and models in Schwalm.  It has a standard deviation greater than 1 while the majority of the points in Schwalm are within 1.  Additionally, the ensemble models RMSE is greater than 1 and in Schwalm only one point, model U, has an RMSE greater than 1.  The correlation of the simple ensemble was .67 which puts it with the majority of benchmarks and models that fall within the 0.6-0.8 correlation sector.  Overall, the simple emsemble does a comparitvely bad job with regards to prediction accuracy and precision as indicated by is relatively large RMSE and standard deviation values.

Time-scales
-----------

Many ecological processes operate at multiple time scales. For example, carbon flux data responds to the diurnal cycle of light and temperature, meso-scale variability due to weather fronts, seasonal variability, and inter-annual variability driven by longer-term climate modes, as well as disturbance and succession.

In the next section we look at the average diurnal cycle of the data and models.

```{r}
## diurnal cycle
NEE.ens.diurnal = tapply(E,L4$Hour[qaqc],mean)
NEE.pf.diurnal  = tapply(P,L4$Hour[qaqc],mean)
NEE.clim.diurnal  = tapply(C,L4$Hour[qaqc],mean)
NEE.obs.diurnal = tapply(O,L4$Hour[qaqc],mean)
ylim=range(c(NEE.ens.diurnal,NEE.pf.diurnal,NEE.obs.diurnal))
tod = sort(unique(L4$Hour))
plot(tod,NEE.ens.diurnal,ylim=ylim,col=2,xlab="Time of Day",ylab='NEE',main="Diurnal Cycle",type='l',lwd=3)
lines(tod,NEE.pf.diurnal,col=3,lwd=3)
lines(tod,NEE.clim.diurnal,col=4,lwd=3)
lines(tod,NEE.obs.diurnal,lwd=3)
legend("bottomright",legend=c("obs","ens","PF","clim"),col=1:4,pch=20,cex=0.75)
```

Question 7: What time of day has the largest uncertainty? What does this suggest about what parameter(s) needs to be modified in the model, in what direction, and by approximately how much? In providing this answer, recall the structure of the model as well as the fact that the particle filter has assimilated LAI so we can assume that that term is unbiased for that case.

> In between the hours of 10 and 15 we observe the greatest uncertainty because all three models diverge from the observations during this time to a degree larger than other hours in the day.  We observed underestimation of NEE by the ensemble and PF model which suggest that parameters related to photosyntheis need to be modified.  This period of the day sees the greatest sunlight intensity, due to SZA, so photosyntheis should be instigating increased carbon uptake.  These models are predicting lower NEE values which means that they believe that the ecosystem is removing more carbon (decreasing the net carbon) than observed.  This underestimation of NEE is due to overestimation of productivity of plants in the ecosystem.  In Lab 10 we used LAI and light to estimate GPP which was then used in the determination of Ra, Leaf NPP, and woody NPP.  Assuming LAI is unbiased we would look to modify GPP.  By decreasing GPP by about half (the obs value of NEE at highest uncertainty is about -7 and the model value of NEE is -17) would shrink this residual.

Bayesian p-values
-----------------

Next, let's look at the patterns in the Bayesian 'p-values', which is essentially a plot of the quantiles of the observed values relative to the predictive distributions. The ideal distribution of quantiles if flat (values show up as frequently as predicted), overcalibrated models tend to have too many observations near 50%, while poorly calibrated models will tend to produce a lot of 0's and 1's (consistently under- or over-predicting). 

```{r}
O = L4$NEE_st_fMDS  ## observed
pval.pf = 0
for(i in 1:nrow(output)){
  pval.pf[i] = sum(O[i] > -output[i,,5])/ncol(output)  ## quantiles of the particle filter
}
plot(pval.pf)   ## quantile 'residuals'
hist(pval.pf,probability=TRUE) ## quantile distribution (should be flat)

pval.ens = 0
for(i in 1:nrow(output.ensemble)){
  pval.ens[i] = sum(O[i] > -output.ensemble[i,,5])/ncol(output.ensemble)  ## quantiles of the ensemble
}
plot(pval.ens)   ## quantile 'residuals'
hist(pval.ens,probability=TRUE) ## quantile distribution (should be flat)
```
Question 8: How do the ensemble and particle filter perform in terms of the predicted quantiles?

> The particle filter produces a lot of 0 and 1's indicating that it consistently over and under predicts.  Looking at the plot of the quantiles of the observed values against the predictive distributions we observe that many of these points fall within the upper quantile (1) or lowest quantile (0).  This is also obsevered in the histogram of the pvalue of pf.  This shows that for NEE PF overpredicts NEE.  This is what we observed above in the Time series plot between hours 10 and 15: the PF predicted values of NEE were much greater than the observed values.  

> For the ensemble model we also observe many of the points in the quantiles of the observed values against the predictive distributions to be 1's or 0's.  To an even greater degree we observe points in the uppermost quantile compared to the PF.  This shows that this model overpredicts more than the PF and underpredicts less than the PR.  The histogram of the pvalue of the ensemble has higher density in bins from values 0.8 to 1 with a slighlty less density in the 1 bin.


Mining the Residuals
--------------------

In the final section we'll use a few off-the-shelf data mining approaches to look at the model residuals and ask what parts of our input space are associated with the largest model error. Note that we are not limited to just examining the effects of the model inputs, we might also look at other potential drivers that are not included in our model, such as soil moisture, to ask if model error is associated with our failure to include this (or other) drivers. Alternatively, we could have looked at other factors such as the time of day or even other model variables (e.g. is model error higher when LAI is larger or small?)

Of the many algorithms out there we'll look at two: the Classification and Regression Tree (CART) model and the Random Forest model. For both we'll define our error metric as $(E-O)/beta$, where beta is the parameter equivalent to the variance in Laplace distribution. Specifically, we're using the heteroskedastic observation error to reweight the residuals to account for the fact that large residuals at times of high flux is likely due to high measurement error. Thus the errors can be interpreted as similar to the number of of standard deviations.

The CART model is a classification algorithm which will build a tree that discretely classifies when the model has high and low error.

The Random Forest model is more like a response surface. The Random Forest will generate 'partial dependence' plots, which indicate the importance of each factor across its range, as well as an overall estimate of the importance of each factor in the model error. 

The key thing to remember in all these plots is that we're modelling the RESIDUALS in order to diagnose errors, not modeling the NEE itself.

```{r}
#photosynthetic activate raditon y is absolute error x is value of par or temp

## define error metric and dependent variables
O = L4$NEE_st_fMDS[qaqc]
err = (E-O)/beta
x = cbind(inputs$PAR[qaqc],inputs$temp[qaqc])
colnames(x) = c("PAR","temp")
smp = sample.int(length(err),1000)  ## take a sample of the data since some alg. are slow

### Classification tree
rpb = rpart(err ~ x) ## bias
plot(rpb)
text(rpb)
e2 = err^2
rpe = rpart(e2 ~ x) ## sq error
plot(rpe)
text(rpe)

## Random Forest
rfe = randomForest(x[smp,],abs(err[smp]))
rfe$importance
partialPlot(rfe,x[smp,],"PAR")
partialPlot(rfe,x[smp,],"temp")
```

Question 9: Overall, which driver is most important in explaining model error? What conditions are most associated with model success? With model failure?  Where do these results reinforce conclusions we reached earlier and where do they shine light on new patterns you may have missed earlier?

> Temperature is more important that Photosyntic active radiation in explaining model error.  Looking at the CART model we observe that the driver whose partitions capture the larger sq.error and bias values is temperature.  The bins for temperature captures sq. error and bias values that are larger than those capture by PAR.  This observation, that temperature bins capture larger sources of error and bias than PAR, in addition to temperature being the first source of data partitioning indicate that it has greater explanatory power in explaining model error.

>  The conditions associate with model success are under values of temperature and PAR that have smaller values of sq.error and bias.  Conditions for small bias are temperatures less than 19 with PAR less than 696 or with a PAR greater than 696 and with a temperature greater than 14.85.  Another condition that could result in small bias is if the temperature is greater than 19 but has a PAR of less than 446. Conditions for small sq.error are temperatures less than 24 and under 18 in particular.  To summarize, temperatures less than 19 and above 14 have the most success followed by temperatures greater than 19 but with a PAR less than 446.

> The conditions associates with model failure are under values of temperature and PAR that have larger values of sq.error and bias.  These conditions for large bias are observed when temperatures are greater than 26 with a PAR greater than 1149.  The conditions for large sq.error are temeratures greater than 24, and in particular greater than 26, and PAR greater than 1149.  

> These results reinforce aforementioned conclusions that model error increases under warmer temperatures and greater PAR.  This is due to the overestimation of GPP we observed earlier.  We found that during the hours of the day with the most sunlight (highest PAR) and warmer temperatures that prediction of NEE was underestimated due to overestimation of GPP.  Looking at the Partial Dependence plots of PAR and Temperature we observe that the absolute error range for temperature is greater than PAR, while PAR's values trend upwards over the range of PAR values.  These plots show that the absolute error of temperature is very low between values of 5 and 19 and very high in values greater than 20, which reinforces our prior conclusions from the CART model.  This observed range in contribution size to model error highlights the sensitivity of the model to this driver and thus reinforces the greater importance of temperature to model error.

Functional Responses
--------------------

In this section we look at how well the model performed by assessing the modeled relationships between inputs and outputs and comparing that to the same relationship in the data. The raw relationships are very noisy, as many covariates are changing beyond just the single input variable we are evaluating, so in addition we calculate binned means for both the model and data.

```{r}
E = NEE.ens[qaqc]
O = L4$NEE_st_fMDS[qaqc]

## raw
plot(inputs$PAR[qaqc],O,pch=".",ylab="NEE")
points(inputs$PAR[qaqc],E,pch=".",col=2)

## binned
nbin = 25
PAR = inputs$PAR[qaqc]
x = seq(min(PAR),max(PAR),length=nbin)
#Tair = inputs$temp[qaqc]
#xd = seq(min(Tair),max(Tair),length=nbin)
xmid = x[-length(x)] + diff(x)
bin = cut(PAR,x)
Obar = tapply(O,bin,mean,na.rm=TRUE)
Ose  = tapply(O,bin,std.error,na.rm=TRUE)
Ebar = tapply(E,bin,mean,na.rm=TRUE)
Ese  = tapply(E,bin,std.error,na.rm=TRUE)
OCI = -cbind(Obar-1.96*Ose,Obar,Obar+1.96*Ose)
ECI = -cbind(Ebar-1.96*Ese,Ebar,Ebar+1.96*Ese)
rng = range(rbind(OCI,ECI))

col2=col.alpha("darkgrey",0.9)
col1=col.alpha("lightgrey",0.6)

plot(xmid,Obar,ylim=rng,type='n',xlab="PAR umol (C)",ylab="NEP (umol/m2/s)",cex.lab=1.3)
ciEnvelope(xmid,ECI[,1],ECI[,3],col=col2)
lines(xmid,ECI[,2],col="red",lwd=4)
ciEnvelope(xmid,OCI[,1],OCI[,3],col=col1)
lines(xmid,OCI[,2],col="lightgrey",lwd=4)

legend("bottom",legend=c("Model","Data"),lwd=10,col=c(col2,col1),lty=1,cex=1.7)

## raw
plot(inputs$temp[qaqc],O,pch=".",ylab="NEE")
points(inputs$temp[qaqc],E,pch=".",col=2)

## binned
nbin = 25
#PAR = inputs$PAR[qaqc]
#x = seq(min(PAR),max(PAR),length=nbin)
Tair = inputs$temp[qaqc]
xd = seq(min(Tair),max(Tair),length=nbin)
xmid = xd[-length(xd)] + diff(xd)
bin = cut(Tair,xd)
Obar = tapply(O,bin,mean,na.rm=TRUE)
Ose  = tapply(O,bin,std.error,na.rm=TRUE)
Ebar = tapply(E,bin,mean,na.rm=TRUE)
Ese  = tapply(E,bin,std.error,na.rm=TRUE)
OCI = -cbind(Obar-1.96*Ose,Obar,Obar+1.96*Ose)
ECI = -cbind(Ebar-1.96*Ese,Ebar,Ebar+1.96*Ese)
rng = range(rbind(OCI,ECI))

col2=col.alpha("darkgrey",0.9)
col1=col.alpha("lightgrey",0.6)

plot(xmid,Obar,ylim=rng,type='n',xlab="Air Temperature (C)",ylab="NEP (umol/m2/s)",cex.lab=1.3)
ciEnvelope(xmid,ECI[,1],ECI[,3],col=col2)
lines(xmid,ECI[,2],col="white",lwd=4)
ciEnvelope(xmid,OCI[,1],OCI[,3],col=col1)
lines(xmid,OCI[,2],col="lightgrey",lwd=4)

legend("bottom",legend=c("Model","Data"),lwd=10,col=c(col2,col1),lty=1,cex=1.7)



```

Question 10: Evaluate the model's ability to capture functional responses to both Temperature and PAR.

> The model is able to capture the functional respone of NEP with temperature. We observe small confidence intervals on on the model for temperature between zero and twenty.  This reinforces our aforemention conclusoins that temperature is the more important driver of model error.  As the temperature exceeds 20 we see the models confidence intervals increasing as temperature increases.  We also see the model diverging from the data as temperature increases which further reinforces these conclusions.  The plot of NEE against the raw temperature data inputs also demonstrates this divergence of the model from the data as temperature increases.  The spread of the temperature inputs from the ensemble model increases as the inputs move down and away from NEE; underestimating NEE due to overestimation of GPP.  This functional response appears to be an S curve which would make it a type 3 response.

> The functional response of PAR is not captured well by the model.  In the raw plot we see that the inputs from the Ensemble have a clear downward trend that underestimates the data across all values of PAR.  This underestimation increases as PAR input values increase.  Looking at the bin plot, the PAR values from the model are a linear line which is a type 1 functional response.  We do not see confidence intervals until the very end of the plot which could be a coding error.  

Overall 
-------

Below is a final summary figure of the model's performance on a daily timescale that combines many of the previous assessments.

```{r}

### other summary figures to go in multi-panel
par(mfrow=c(2,2))

## Time-series visualization, daily means
DoY = floor(L4$DoY-0.02)
uDoY = sort(unique(DoY))
i=5
ci.pf  = apply(apply(output[,,i],2,tapply,DoY,mean),1,mean)
NEE = -L4$NEE_st_fMDS
NEEd = tapply(NEE,DoY,mean)
plot(uDoY,ci.pf,xlab="time",ylab=units[i],type='l',ylim=range(c(ci.pf,NEEd)),cex.lab=1.3)
points(uDoY,NEEd,col=2,pch="+")
legend("topright",legend=c("Model","Data"),lty=c(1,NA),pch=c(NA,"+"),col=1:2,cex=1.3)

## predicted vs observed
plot(NEEd,ci.pf,xlab="Model",ylab="Data",cex.lab=1.3)
abline(0,1,lty=2,lwd=4)
abline(lm(ci.pf ~ NEEd),col=2,lwd=3,lty=3)
legend("topleft",legend=c("1:1","Reg"),lty=2:3,lwd=4,col=1:2,cex=1.3)

## Functional response
plot(xmid,Obar,ylim=rng,type='n',xlab="Air Temperature (C)",ylab="NEP (umol/m2/s)",cex.lab=1.3)
ciEnvelope(xmid,ECI[,1],ECI[,3],col=col2)
lines(xmid,ECI[,2],col="white",lwd=4)
ciEnvelope(xmid,OCI[,1],OCI[,3],col=col1)
lines(xmid,OCI[,2],col="lightgrey",lwd=4)

legend("bottom",legend=c("Model","Data"),lwd=10,col=c(col2,col1),lty=1,cex=1.3)

### Classification tree
par(mar=c(0,0,0,0))
rpe = rpart(e2 ~ PAR+Tair,as.data.frame(x),method="anova") ## sq error
plot(rpe,margin=0.1)
text(rpe,cex=1.5)

```



