---
title: 'GE 585 HO #5'
author: "Casey Kelly"
date: "February 21, 2019"
output: html_document
---
### Activity Task 1
Run the unknown mean/fixed variance model to estimate mean tree diameter. Include the following in your results:


Establishing unknown mean/fixed variance model and data/
```{r}
library(rjags)
library(coda)

NormalMean <- "
model {
  mu ~ dnorm(mu0,T) # prior on the mean 
  for(i in 1:N){
    X[i] ~ dnorm(mu,S) # data model
  }
}
"
data = list(
  N = 297, #data
  mu0=20, #priod mean
  T=0.01, #data preceision
  S = 1/27,  #priod precison
  X = c(20.9, 13.6, 15.7, 6.3, 2.7, 25.6, 4, 20.9, 7.8, 27.1, 25.2, 19, 17.8, 22.8, 12.5, 21.1, 22, 22.4, 5.1, 16, 20.7, 15.7, 5.5, 18.9, 22.9, 15.5, 18.6, 19.3, 14.2, 12.3, 11.8, 26.8, 17, 5.7, 12, 19.8, 19, 23.6, 19.9, 8.4, 22, 18.1, 21.6, 17, 12.4, 2.9, 22.6, 20.8, 18.2, 14.2, 17.3, 14.5, 8.6, 9.1, 2.6, 19.8, 20, 22.2, 10.2, 12.9, 20.9, 21.1, 7.3, 5.8, 23.1, 17, 21.5, 10.1, 18.4, 22.6, 21.2, 21.5, 22.4, 17.3, 16, 25, 22.4, 23.9, 23, 21.9, 19, 28.6, 16, 22.5, 23.2, 8.7, 23.4, 15.3, 25.6, 19.2, 17.4, 23.8, 20.4, 19, 3.6, 23.4, 19.6, 17.5, 16.5, 22, 19.7, 7.35, 18, 17.8, 9.6, 15, 12, 17.7, 21.4, 17, 22.1, 18.9, 15.05, 12.9, 19.3, 15.3, 13.6, 15.4, 10.6, 11.3, 11.8, 22.2, 22.2, 13.1, 7.4, 4.5, 11.7, 19.5, 19.9, 11.6, 13.9, 15.5, 11, 18.6, 17.6, 12.7, 20.9, 18.8, 22.4, 21.2, 18.2, 15.3, 13.6, 7.3, 17.4, 17.4, 10.5, 22.9, 23.2, 13.8, 14.8, 22.2, 20.9, 13, 18.9, 19, 15.2, 16.8, 18, 24.6, 15.4, 17.2, 23.2, 22.8, 25.5, 7.8, 6, 6.4, 19, 13.5, 23.7, 18, 22.2, 22.4, 9.3, 13.7, 18.9, 20.5, 23.3, 20.8, 18.4, 4.5, 12.2, 16.9, 13.5, 17.8, 16.9, 20.4, 19.5, 22.2, 24.5, 21.2, 16.5, 18, 16.4, 3.9, 17.9, 22, 12.9, 21, 18, 9.2, 15.9, 8.1, 8.3, 10.7, 12, 19.9, 13.6, 17.3, 11.5, 12.4, 15.1, 22, 19.3, 17.5, 14.5, 14.7, 17.5, 19.6, 12.9, 20.3, 17.9, 20.2, 18.3, 9.5, 19, 21, 13.1, 20.4, 16.3, 18.3, 11.8, 23.3, 15.2, 20, 17.9, 12, 19.6, 18.5, 16.2, 10.9, 17.8, 13.8, 10, 17.9, 15.6, 20.3, 14.9, 18.6, 12.5, 18.2, 16, 18.7, 18, 15.3, 19, 17.9, 15.8, 17.7, 14.4, 19.6, 18.3, 18.7, 17.8, 18, 10.1, 18.8, 16.4, 21.2, 16.6, 16.7, 17.8, 16.5, 19.3, 16.3, 14.2, 13, 9.4, 19.7, 13.4, 2.6, 17.6, 16.7, 17.6, 5.8, 17.6, 20.1, 18.2, 16.7, 14, 13.9, 5.1, 16.6, 3.9, 17.5, 18)
) 

```


Now create model in jags and get output.
```{r}
j.model   <- jags.model (file = textConnection(NormalMean), #putting model and data in jags
                         data = data,
                         n.chains = 5)
jags.out   <- coda.samples (model = j.model, #sampling from model
                            variable.names = c("mu"),
                            n.iter = 3000)
```
> After first running the model with 3 chains and 1000 iterations and not reaching convergence, I increased the chains to 5 and iterations to 2000 and then 3000 until convergence was reached. I determined convergenced was reached by plotting diagnostics such as MCMC history (Trace MU) and parameter density (Density of mu).  I also used the Brooks-Gelman-Rubin (BGR) statistic to more objective determine model convergence.  The point estimate and model confidence interval were both 1 and the BGR plot converged.  Using these diagnostics I determined the burn in period to be the first 200 samples so I removed those from the model.

Now do visualize and diagnostics checks for convergence
```{r}
plot(jags.out) # check for convergence

gelman.diag(jags.out) #convergence test statistic

BGR <- gelman.plot(jags.out) #finding samples before convergenc to find "burn in"

burnin = 200                               ## determine convergence
jags.burn <- window(jags.out,start=burnin)  ## remove burn-in
plot(jags.burn)                             ## check diagnostics post burn-in
gelman.plot(jags.burn)
```

```{r}
acfplot(jags.burn) #autocorrelation plot; looking for fast decay to zero fas indicator of independence
effectiveSize(jags.burn) #15335.65 is the effective sample size, stable sample size for mean
cumuplot(jags.burn,probs=c(0.025,0.25,0.5,0.75,0.975))
```

Summary Statistics for posterior of the Mean
```{r}
summary(jags.burn)
```


#Task 2

Establishing priors on mean (mu) and data precision (S) 
```{r}
NormalMean <- "
model {
mu ~ dnorm(mu0,T) # prior on the mean 
S ~ dnorm(S0, 1/27)
for(i in 1:N){
X[i] ~ dnorm(mu,S) # data model
}
}
"
sigma=.3 #prior st. dev from task 1
data = list(
  N = 297, #data
  mu0=20, #priod mean
  T=0.01, #data preceision
  S0 = 1/sigma^2,  #priod precison
  X = c(20.9, 13.6, 15.7, 6.3, 2.7, 25.6, 4, 20.9, 7.8, 27.1, 25.2, 19, 17.8, 22.8, 12.5, 21.1, 22, 22.4, 5.1, 16, 20.7, 15.7, 5.5, 18.9, 22.9, 15.5, 18.6, 19.3, 14.2, 12.3, 11.8, 26.8, 17, 5.7, 12, 19.8, 19, 23.6, 19.9, 8.4, 22, 18.1, 21.6, 17, 12.4, 2.9, 22.6, 20.8, 18.2, 14.2, 17.3, 14.5, 8.6, 9.1, 2.6, 19.8, 20, 22.2, 10.2, 12.9, 20.9, 21.1, 7.3, 5.8, 23.1, 17, 21.5, 10.1, 18.4, 22.6, 21.2, 21.5, 22.4, 17.3, 16, 25, 22.4, 23.9, 23, 21.9, 19, 28.6, 16, 22.5, 23.2, 8.7, 23.4, 15.3, 25.6, 19.2, 17.4, 23.8, 20.4, 19, 3.6, 23.4, 19.6, 17.5, 16.5, 22, 19.7, 7.35, 18, 17.8, 9.6, 15, 12, 17.7, 21.4, 17, 22.1, 18.9, 15.05, 12.9, 19.3, 15.3, 13.6, 15.4, 10.6, 11.3, 11.8, 22.2, 22.2, 13.1, 7.4, 4.5, 11.7, 19.5, 19.9, 11.6, 13.9, 15.5, 11, 18.6, 17.6, 12.7, 20.9, 18.8, 22.4, 21.2, 18.2, 15.3, 13.6, 7.3, 17.4, 17.4, 10.5, 22.9, 23.2, 13.8, 14.8, 22.2, 20.9, 13, 18.9, 19, 15.2, 16.8, 18, 24.6, 15.4, 17.2, 23.2, 22.8, 25.5, 7.8, 6, 6.4, 19, 13.5, 23.7, 18, 22.2, 22.4, 9.3, 13.7, 18.9, 20.5, 23.3, 20.8, 18.4, 4.5, 12.2, 16.9, 13.5, 17.8, 16.9, 20.4, 19.5, 22.2, 24.5, 21.2, 16.5, 18, 16.4, 3.9, 17.9, 22, 12.9, 21, 18, 9.2, 15.9, 8.1, 8.3, 10.7, 12, 19.9, 13.6, 17.3, 11.5, 12.4, 15.1, 22, 19.3, 17.5, 14.5, 14.7, 17.5, 19.6, 12.9, 20.3, 17.9, 20.2, 18.3, 9.5, 19, 21, 13.1, 20.4, 16.3, 18.3, 11.8, 23.3, 15.2, 20, 17.9, 12, 19.6, 18.5, 16.2, 10.9, 17.8, 13.8, 10, 17.9, 15.6, 20.3, 14.9, 18.6, 12.5, 18.2, 16, 18.7, 18, 15.3, 19, 17.9, 15.8, 17.7, 14.4, 19.6, 18.3, 18.7, 17.8, 18, 10.1, 18.8, 16.4, 21.2, 16.6, 16.7, 17.8, 16.5, 19.3, 16.3, 14.2, 13, 9.4, 19.7, 13.4, 2.6, 17.6, 16.7, 17.6, 5.8, 17.6, 20.1, 18.2, 16.7, 14, 13.9, 5.1, 16.6, 3.9, 17.5, 18)
) 

```

>For my choice of prior on the precision I used the standard deviation on the posterior mean from the first task to create my distribution on S for this task.  

Now create model in jags and get output.
```{r}
j.model2   <- jags.model (file = textConnection(NormalMean), #putting model and data in jags
                         data = data,
                         n.chains = 5)
jags.out2   <- coda.samples (model = j.model2, #sampling from model
                            variable.names = c("mu",'S'),
                            n.iter = 5000)
```

> For the MCMC I started with the number of chains and interations (5 and 3000 respectfuly) from task 1. After running diagnostic statistics and looking at convergence plots I increaed the interations gradually, increments of 500, until I had both diagnostic convergence statistics and plots that were satisfactory for both variables, S and mu.  Looking on the BGR plots I selected a burnin of 1200 and removed those samples from the model.

Now do visualize and diagnostics checks for convergence
```{r}
plot(jags.out2) # check for convergence

gelman.diag(jags.out2) #convergence test statistic

GBR <- gelman.plot(jags.out2) #finding samples before convergenc to find "burn in"
burnin = 1200                               ## determine convergence
jags.burn2 <- window(jags.out2,start=burnin)  ## remove burn-in
plot(jags.burn2)                             ## check diagnostics post burn-in
gelman.plot(jags.burn2)
```

```{r}
acfplot(jags.burn2) #autocorrelation plot; looking for fast decay to zero fas indicator of independence
effectiveSize(jags.burn2) #15335.65 is the effective sample size, large enough for mean
cumuplot(jags.burn2,probs=c(0.025,0.25,0.5,0.75,0.975))
```

Summary Statistics for posterior of the Mean
```{r}
summary(jags.burn2)
```


Comparison of mean (shape, location, tails)
```{r}
plot(jags.burn)                             ## check diagnostics post burn-in
gelman.plot(jags.burn)
plot(jags.burn2)                             ## check diagnostics post burn-in
gelman.plot(jags.burn2)
summary(jags.burn)
summary(jags.burn2)
```
> Looking at the dianostic plots and the summary statistics on the mean I do not observe any stark distributional changes in the mean.  The quantiles, SD, and Time-series SE are very close, within a hundredth of each other. This supports my conclusion from my visual comparison that the shape, location, and tails for each distribution on very similar.  These similarities could be attributed to my selection for prior on S as the standard deviation of the mean from the first task.